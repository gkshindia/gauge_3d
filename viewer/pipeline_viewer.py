"""
Pipeline Overview Visualization Module

Provides comprehensive visualization of the entire 3D reconstruction pipeline,
showing relationships between different stages and their outputs.
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from typing import List, Dict, Tuple, Optional, Union, Any
import logging
from datetime import datetime
import seaborn as sns

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    HAS_PLOTLY = True
except ImportError:
    HAS_PLOTLY = False

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False


class PipelineViewer:
    """Comprehensive pipeline visualization and analysis tools."""
    
    def __init__(self, output_dir: str = "output"):
        """
        Initialize the PipelineViewer.
        
        Args:
            output_dir: Base output directory containing pipeline outputs
        """
        self.output_dir = Path(output_dir)
        self.pipeline_stages = {
            'frames': self.output_dir / "frames",
            'depth_maps': self.output_dir / "depth_maps", 
            'depth_previews': self.output_dir / "depth_previews",
            'depth_stats': self.output_dir / "depth_stats",
            'point_clouds': self.output_dir / "gaussian_reconstruction" / "point_clouds",
            'gaussian_reconstruction': self.output_dir / "gaussian_reconstruction",
            'logs': self.output_dir / "gaussian_reconstruction" / "logs"
        }
    
    def scan_pipeline_outputs(self) -> Dict[str, Dict]:
        """
        Scan all pipeline outputs and collect metadata.
        
        Returns:
            Dictionary containing metadata for each pipeline stage
        """
        pipeline_data = {}
        
        for stage_name, stage_path in self.pipeline_stages.items():
            if not stage_path.exists():
                pipeline_data[stage_name] = {
                    'exists': False,
                    'file_count': 0,
                    'total_size': 0,
                    'files': []
                }
                continue
            
            files = []
            total_size = 0
            
            if stage_path.is_file():
                # Single file
                files = [stage_path]
                total_size = stage_path.stat().st_size
            else:
                # Directory
                for file_path in stage_path.rglob('*'):
                    if file_path.is_file():
                        files.append(file_path)
                        total_size += file_path.stat().st_size
            
            pipeline_data[stage_name] = {
                'exists': True,
                'file_count': len(files),
                'total_size': total_size,
                'files': [str(f.relative_to(self.output_dir)) for f in files[:100]],  # Limit for display
                'path': str(stage_path.relative_to(self.output_dir))
            }
        
        return pipeline_data
    
    def create_pipeline_overview(self, save_path: Optional[Union[str, Path]] = None) -> plt.Figure:
        """
        Create a comprehensive pipeline overview visualization.
        
        Args:
            save_path: Optional path to save the figure
            
        Returns:
            Matplotlib figure
        """
        pipeline_data = self.scan_pipeline_outputs()
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('3D Reconstruction Pipeline Overview', fontsize=16, fontweight='bold')
        
        # 1. File count by stage
        ax1 = axes[0, 0]
        stages = []
        file_counts = []
        colors = []
        
        for stage, data in pipeline_data.items():
            if data['exists'] and data['file_count'] > 0:
                stages.append(stage.replace('_', '\n'))
                file_counts.append(data['file_count'])
                colors.append('green' if data['file_count'] > 0 else 'red')
        
        bars = ax1.bar(stages, file_counts, color=colors, alpha=0.7)
        ax1.set_title('Files Generated by Stage')
        ax1.set_ylabel('Number of Files')
        ax1.tick_params(axis='x', rotation=45)
        
        # Add value labels on bars
        for bar, count in zip(bars, file_counts):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(file_counts)*0.01,
                    f'{count}', ha='center', va='bottom')
        
        # 2. Data size by stage
        ax2 = axes[0, 1]
        sizes_mb = []
        for stage, data in pipeline_data.items():
            if data['exists']:
                sizes_mb.append(data['total_size'] / (1024 * 1024))  # Convert to MB
        
        if sizes_mb:
            wedges, texts, autotexts = ax2.pie(sizes_mb, labels=[s.replace('_', '\n') for s in stages], 
                                             autopct='%1.1f%%', startangle=90)
            ax2.set_title('Data Size Distribution (MB)')
        
        # 3. Pipeline flow diagram
        ax3 = axes[1, 0]
        self._draw_pipeline_flow(ax3, pipeline_data)
        
        # 4. Processing statistics
        ax4 = axes[1, 1]
        self._plot_processing_stats(ax4, pipeline_data)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Pipeline overview saved to {save_path}")
        
        return fig
    
    def _draw_pipeline_flow(self, ax, pipeline_data: Dict):
        """Draw a pipeline flow diagram."""
        # Define pipeline flow
        flow_stages = [
            ('Input Video', 'frames'),
            ('Depth Estimation', 'depth_maps'),
            ('Point Cloud\nGeneration', 'point_clouds'),
            ('4D Gaussian\nReconstruction', 'gaussian_reconstruction')
        ]
        
        # Create a simple flow diagram
        y_positions = np.linspace(0.8, 0.2, len(flow_stages))
        x_position = 0.5
        
        for i, (stage_name, stage_key) in enumerate(flow_stages):
            # Determine color based on stage completion
            if stage_key in pipeline_data and pipeline_data[stage_key]['exists']:
                color = 'lightgreen' if pipeline_data[stage_key]['file_count'] > 0 else 'lightcoral'
                status = '✓' if pipeline_data[stage_key]['file_count'] > 0 else '✗'
            else:
                color = 'lightgray'
                status = '?'
            
            # Draw stage box
            rect = plt.Rectangle((x_position - 0.15, y_positions[i] - 0.05), 
                               0.3, 0.1, facecolor=color, edgecolor='black', linewidth=1)
            ax.add_patch(rect)
            
            # Add stage text
            ax.text(x_position, y_positions[i], f'{stage_name}\n{status}', 
                   ha='center', va='center', fontsize=10, fontweight='bold')
            
            # Draw arrow to next stage
            if i < len(flow_stages) - 1:
                ax.arrow(x_position, y_positions[i] - 0.05, 0, 
                        y_positions[i+1] - y_positions[i] + 0.1, 
                        head_width=0.02, head_length=0.02, fc='black', ec='black')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.set_title('Pipeline Flow Status')
        ax.axis('off')
    
    def _plot_processing_stats(self, ax, pipeline_data: Dict):
        """Plot processing statistics if available."""
        # Try to load processing logs or stats
        stats_file = self.output_dir / "depth_stats"
        
        if stats_file.exists():
            # Look for depth processing stats
            stats_files = list(stats_file.glob("*_depth_results.json"))
            if stats_files:
                try:
                    with open(stats_files[0], 'r') as f:
                        depth_stats = json.load(f)
                    
                    # Plot depth statistics
                    metrics = ['mean_depth', 'std_depth', 'min_depth', 'max_depth']
                    values = [depth_stats.get(metric, 0) for metric in metrics]
                    
                    bars = ax.bar(metrics, values, color='skyblue', alpha=0.7)
                    ax.set_title('Depth Processing Statistics')
                    ax.set_ylabel('Depth Value')
                    ax.tick_params(axis='x', rotation=45)
                    
                    # Add value labels
                    for bar, value in zip(bars, values):
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                               f'{value:.2f}', ha='center', va='bottom')
                    
                except Exception as e:
                    ax.text(0.5, 0.5, f'Error loading stats:\n{str(e)}', 
                           ha='center', va='center', transform=ax.transAxes)
            else:
                ax.text(0.5, 0.5, 'No depth statistics found', 
                       ha='center', va='center', transform=ax.transAxes)
        else:
            ax.text(0.5, 0.5, 'No processing statistics available', 
                   ha='center', va='center', transform=ax.transAxes)
        
        ax.set_title('Processing Statistics')
    
    def create_interactive_pipeline_dashboard(self) -> Optional[go.Figure]:
        """
        Create an interactive dashboard showing pipeline status and metrics.
        
        Returns:
            Plotly figure or None if Plotly not available
        """
        if not HAS_PLOTLY:
            logging.warning("Plotly not available for interactive dashboard")
            return None
        
        pipeline_data = self.scan_pipeline_outputs()
        
        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('File Counts by Stage', 'Data Sizes (MB)', 
                           'Pipeline Timeline', 'Stage Details'),
            specs=[[{"type": "bar"}, {"type": "pie"}],
                   [{"type": "scatter"}, {"type": "table"}]]
        )
        
        # 1. File counts bar chart
        stages = []
        file_counts = []
        for stage, data in pipeline_data.items():
            if data['exists']:
                stages.append(stage.replace('_', ' ').title())
                file_counts.append(data['file_count'])
        
        fig.add_trace(
            go.Bar(x=stages, y=file_counts, name="File Count"),
            row=1, col=1
        )
        
        # 2. Data sizes pie chart
        sizes_mb = [data['total_size'] / (1024 * 1024) for data in pipeline_data.values() if data['exists']]
        if sizes_mb:
            fig.add_trace(
                go.Pie(labels=stages, values=sizes_mb, name="Data Size"),
                row=1, col=2
            )
        
        # 3. Create a timeline view
        stage_order = ['frames', 'depth_maps', 'point_clouds', 'gaussian_reconstruction']
        timeline_x = []
        timeline_y = []
        timeline_status = []
        
        for i, stage in enumerate(stage_order):
            if stage in pipeline_data:
                timeline_x.append(i)
                timeline_y.append(pipeline_data[stage]['file_count'])
                timeline_status.append('Complete' if pipeline_data[stage]['file_count'] > 0 else 'Incomplete')
        
        fig.add_trace(
            go.Scatter(x=timeline_x, y=timeline_y, mode='markers+lines',
                      marker=dict(size=10), name="Processing Flow"),
            row=2, col=1
        )
        
        # 4. Details table
        table_headers = ['Stage', 'Files', 'Size (MB)', 'Status']
        table_rows = []
        
        for stage, data in pipeline_data.items():
            status = 'Complete' if data['exists'] and data['file_count'] > 0 else 'Missing/Empty'
            size_mb = data['total_size'] / (1024 * 1024) if data['exists'] else 0
            table_rows.append([
                stage.replace('_', ' ').title(),
                data['file_count'],
                f"{size_mb:.2f}",
                status
            ])
        
        fig.add_trace(
            go.Table(
                header=dict(values=table_headers),
                cells=dict(values=list(zip(*table_rows)))
            ),
            row=2, col=2
        )
        
        fig.update_layout(
            title="3D Reconstruction Pipeline Dashboard",
            height=800,
            showlegend=False
        )
        
        return fig
    
    def generate_pipeline_report(self, output_path: Optional[Union[str, Path]] = None) -> str:
        """
        Generate a comprehensive text report of the pipeline status.
        
        Args:
            output_path: Optional path to save the report
            
        Returns:
            Report text
        """
        pipeline_data = self.scan_pipeline_outputs()
        
        report_lines = [
            "3D RECONSTRUCTION PIPELINE REPORT",
            "=" * 50,
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Output Directory: {self.output_dir}",
            "",
            "PIPELINE STAGE SUMMARY",
            "-" * 30
        ]
        
        total_files = 0
        total_size = 0
        
        for stage, data in pipeline_data.items():
            status = "✓ COMPLETE" if data['exists'] and data['file_count'] > 0 else "✗ MISSING/EMPTY"
            size_mb = data['total_size'] / (1024 * 1024) if data['exists'] else 0
            
            report_lines.extend([
                f"",
                f"Stage: {stage.replace('_', ' ').title()}",
                f"  Status: {status}",
                f"  Files: {data['file_count']:,}",
                f"  Size: {size_mb:.2f} MB",
                f"  Path: {data.get('path', 'N/A')}"
            ])
            
            if data['exists']:
                total_files += data['file_count']
                total_size += data['total_size']
                
                # Show sample files
                if data['files']:
                    report_lines.append("  Sample files:")
                    for file_path in data['files'][:5]:
                        report_lines.append(f"    - {file_path}")
                    if len(data['files']) > 5:
                        report_lines.append(f"    ... and {len(data['files']) - 5} more")
        
        report_lines.extend([
            "",
            "OVERALL SUMMARY",
            "-" * 20,
            f"Total Files: {total_files:,}",
            f"Total Size: {total_size / (1024 * 1024):.2f} MB",
            f"Stages Complete: {sum(1 for d in pipeline_data.values() if d['exists'] and d['file_count'] > 0)}/{len(pipeline_data)}",
            "",
            "RECOMMENDATIONS",
            "-" * 15
        ])
        
        # Add recommendations based on pipeline state
        missing_stages = [stage for stage, data in pipeline_data.items() 
                         if not data['exists'] or data['file_count'] == 0]
        
        if missing_stages:
            report_lines.append("Missing or empty stages:")
            for stage in missing_stages:
                report_lines.append(f"  - {stage.replace('_', ' ').title()}")
        else:
            report_lines.append("✓ All pipeline stages have outputs!")
        
        report_text = "\n".join(report_lines)
        
        if output_path:
            with open(output_path, 'w') as f:
                f.write(report_text)
            print(f"Pipeline report saved to {output_path}")
        
        return report_text
    
    def visualize_data_flow(self) -> Optional[plt.Figure]:
        """
        Visualize data flow between pipeline stages using network graph.
        
        Returns:
            Matplotlib figure or None if NetworkX not available
        """
        if not HAS_NETWORKX:
            logging.warning("NetworkX not available for data flow visualization")
            return None
        
        # Create network graph
        G = nx.DiGraph()
        
        # Add nodes and edges for pipeline flow
        flow_edges = [
            ('Input Video', 'Frame Extraction'),
            ('Frame Extraction', 'Depth Estimation'),
            ('Depth Estimation', 'Point Cloud Generation'),
            ('Point Cloud Generation', '4D Gaussian Reconstruction'),
            ('Depth Estimation', 'Depth Analysis'),
        ]
        
        G.add_edges_from(flow_edges)
        
        # Create visualization
        fig, ax = plt.subplots(figsize=(12, 8))
        
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # Draw network
        nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                              node_size=3000, alpha=0.8, ax=ax)
        nx.draw_networkx_edges(G, pos, edge_color='gray', 
                              arrows=True, arrowsize=20, ax=ax)
        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', ax=ax)
        
        ax.set_title('3D Reconstruction Pipeline Data Flow', fontsize=14, fontweight='bold')
        ax.axis('off')
        
        return fig
    
    def compare_processing_runs(self, run_directories: List[Union[str, Path]], 
                              labels: Optional[List[str]] = None) -> plt.Figure:
        """
        Compare multiple processing runs side by side.
        
        Args:
            run_directories: List of output directories to compare
            labels: Optional labels for each run
            
        Returns:
            Matplotlib figure with comparison
        """
        if labels is None:
            labels = [f"Run {i+1}" for i in range(len(run_directories))]
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Pipeline Runs Comparison', fontsize=16, fontweight='bold')
        
        run_data = []
        for run_dir in run_directories:
            # Temporarily change output dir to scan this run
            original_output_dir = self.output_dir
            self.output_dir = Path(run_dir)
            
            # Update pipeline stages paths
            for stage_name in self.pipeline_stages:
                if stage_name == 'frames':
                    self.pipeline_stages[stage_name] = self.output_dir / "frames"
                elif stage_name == 'depth_maps':
                    self.pipeline_stages[stage_name] = self.output_dir / "depth_maps"
                # ... update other paths as needed
            
            pipeline_data = self.scan_pipeline_outputs()
            run_data.append(pipeline_data)
            
            # Restore original output dir
            self.output_dir = original_output_dir
        
        # Plot comparisons
        # 1. File counts comparison
        ax1 = axes[0, 0]
        stages = list(run_data[0].keys())
        x = np.arange(len(stages))
        width = 0.8 / len(run_data)
        
        for i, (data, label) in enumerate(zip(run_data, labels)):
            file_counts = [data[stage]['file_count'] for stage in stages]
            ax1.bar(x + i * width, file_counts, width, label=label, alpha=0.8)
        
        ax1.set_xlabel('Pipeline Stages')
        ax1.set_ylabel('Number of Files')
        ax1.set_title('File Counts Comparison')
        ax1.set_xticks(x + width * (len(run_data) - 1) / 2)
        ax1.set_xticklabels([s.replace('_', '\n') for s in stages], rotation=45)
        ax1.legend()
        
        # 2. Data sizes comparison
        ax2 = axes[0, 1]
        for i, (data, label) in enumerate(zip(run_data, labels)):
            sizes_mb = [data[stage]['total_size'] / (1024 * 1024) for stage in stages]
            ax2.bar(x + i * width, sizes_mb, width, label=label, alpha=0.8)
        
        ax2.set_xlabel('Pipeline Stages')
        ax2.set_ylabel('Data Size (MB)')
        ax2.set_title('Data Sizes Comparison')
        ax2.set_xticks(x + width * (len(run_data) - 1) / 2)
        ax2.set_xticklabels([s.replace('_', '\n') for s in stages], rotation=45)
        ax2.legend()
        
        # 3. Completion status
        ax3 = axes[1, 0]
        completion_data = []
        for data, label in zip(run_data, labels):
            completed = sum(1 for stage_data in data.values() 
                          if stage_data['exists'] and stage_data['file_count'] > 0)
            total = len(data)
            completion_data.append(completed / total * 100)
        
        bars = ax3.bar(labels, completion_data, color='lightgreen', alpha=0.8)
        ax3.set_ylabel('Completion %')
        ax3.set_title('Pipeline Completion Comparison')
        ax3.set_ylim(0, 100)
        
        # Add percentage labels
        for bar, pct in zip(bars, completion_data):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{pct:.1f}%', ha='center', va='bottom')
        
        # 4. Summary table
        ax4 = axes[1, 1]
        ax4.axis('tight')
        ax4.axis('off')
        
        table_data = []
        headers = ['Run'] + [s.replace('_', ' ').title() for s in stages]
        
        for label, data in zip(labels, run_data):
            row = [label]
            for stage in stages:
                status = '✓' if data[stage]['exists'] and data[stage]['file_count'] > 0 else '✗'
                row.append(f"{status} ({data[stage]['file_count']})")
            table_data.append(row)
        
        table = ax4.table(cellText=table_data, colLabels=headers, 
                         cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1.2, 1.5)
        ax4.set_title('Detailed Status Comparison')
        
        plt.tight_layout()
        return fig
